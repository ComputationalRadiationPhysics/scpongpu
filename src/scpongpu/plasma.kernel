#include <cmath> // actually just CUDA Math 
#include "vector.hpp"
#include "parameters.hpp"
#include "utilities.hpp"


#ifndef PLASMA_CU
#define PLASMA_CU

// declare forces:
__forceinline__ __device__ vec ext_force(const vec& location, const vec& speed);
__forceinline__ __device__  vec coulomb_force(const vec& loc1, const vec& loc2);


template<int threads,class Type>
__device__ void copy(Type* destMem,const Type* srcMem, const int threadId)
{
    typedef typename Type::NativeType NativeType;
    for(int i=threadId;i<(threads*sizeof(Type)/sizeof(NativeType));i+=threads)
        ((NativeType*)destMem)[i]=((const NativeType*)srcMem)[i];
    __syncthreads();
}
// usage:
// repace: vec my_speed = speed[index]; /// 1* GLOBAL MEM
//with:
  //copy<parameters::blocksize>(location_s, speed+blockindex, thread_id);
  //vec my_speed = location_s[thread_id];
  //__syncthreads();
// no performance boost



template<class Type>
__device__ void set(Type& value,const Type* srcMem,const int threadId)
{
        typedef typename Type::NativeType NativeType;
#pragma unroll 5
        for(int i=0;i<(sizeof(Type)/sizeof(NativeType));++i)    
            *(((NativeType*)(&value))+i)=*(((const NativeType*)(srcMem+threadId))+i);
        __syncthreads();
}


#if __CUDA_ARCH__ >= 200
__launch_bounds__(parameters::blocksize, parameters::min_blockspermp)
#endif
__global__ void next_accel(vec* location, vec* speed, vec* accel ,const int N_particle, 
				const numtype delta)
{
  const int thread_id = threadIdx.x;  /// 1 * GLOBAL MEM | register=4 
  const int blockindex = parameters::blocksize * blockIdx.x;
  const int index = blockindex + thread_id; /// 1 * GLOBAL MEM | register = 8
  __shared__ vec location_s[parameters::blocksize]; /// shared = blocksize*3*4 = 348 byte

  location_s[thread_id] = location[index]; /// 1 * GLOBAL MEM 
  vec my_location = location_s[thread_id]; /// 1 * GLOBAL MEM
  __syncthreads(); // because  of location_s

  vec a_t0;  //initalizes as  (0,0,0) automaticly
  
  // Coulomb forces:
  for(int read_block = 0; read_block<N_particle/parameters::blocksize; ++read_block)
    {
      location_s[thread_id] = location[(index + read_block*parameters::blocksize)%N_particle]; /// 1 * GLOBAL MEM
      __syncthreads();
      for(int i =0; i<parameters::blocksize; ++i) /// register = ? 8
	a_t0 += coulomb_force(my_location, location_s[i]); /// 22 FLOP | register = 20 
      __syncthreads();
      
    }
  accel[index] = a_t0;  // get friction and harmonic force
                                                           /// 9 FLOP 
}
  


#if __CUDA_ARCH__ >= 200
__launch_bounds__(parameters::blocksize, parameters::min_blockspermp)
#endif
__global__ void next_step_1(vec* location, vec* speed, vec* accel, const int N_particle, 
				const numtype delta)
//first half of velocity verlet algorithm
{
  const int index = parameters::blocksize * blockIdx.x + threadIdx.x; /// 1 * GLOBAL MEM | register = 8

  vec my_location = location[index]; /// 1 * GLOBAL MEM
  vec my_speed = speed[index]; /// 1* GLOBAL MEM
  const vec a_t0 = accel[index] + ext_force(my_location, my_speed); 

  my_speed     +=  a_t0 * (0.5f*delta);  /// 7 FLOP
  my_location  +=  my_speed*delta; /// 6 FLOP
      
  // need to wait for all threads in all blocks to finish 
  // this is not pOssible with CUDA, therefor write all to global memory
  location[index] = my_location;  /// 1 * GLOBAL MEM
  speed[index] = my_speed;   /// 1 * GLOBAL MEM
    
  __syncthreads(); // not quite shure if realy neccesairy
}


#if __CUDA_ARCH__ >= 200
__launch_bounds__(parameters::blocksize, parameters::min_blockspermp)
#endif
__global__ void next_step_2(vec* location, vec* speed, vec* accel, const int N_particle, 
				const numtype delta)
//second half of velocity verlet algorithm
{
  const int index = parameters::blocksize * blockIdx.x + threadIdx.x; /// 1 * GLOBAL MEM | register = 8
  
  vec my_location = location[index]; /// 1 * GLOBAL MEM
  vec my_speed = speed[index]; /// 1* GLOBAL MEM
  const vec a_t0 = accel[index] + ext_force(my_location, my_speed); 

  my_speed += a_t0 *(0.5f*delta);  /// 7 FLOP
  speed[index] = my_speed; /// 1 * GLOBAL MEM

  __syncthreads(); // not quite shure if realy neccesairy
}









// harmonic and friction forces  /// 9 FLOP 
__forceinline__ __device__ vec ext_force(const vec& location, const vec& speed)
{
  const numtype harmonic_x = parameters::harmonic_x; // not contributing to register
  const numtype harmonic_y = parameters::harmonic_y; // not contributing to register
  const numtype harmonic_z = parameters::harmonic_z; // not contributing to register
  const numtype friction = parameters::friction;

  return vec(-1.0f*location.x*harmonic_x, -1.0f*location.y*harmonic_y, -1.0f*location.z*harmonic_z) 
    + speed*(-1.0f*friction); // 9 FLOP
}


// Coulomb force /// 22 FLOP
__forceinline__ __device__  vec coulomb_force(const vec& loc1, const vec& loc2)
{
  const numtype coulomb = parameters::coulomb; /// not contributing to register
  const numtype epsilon = parameters::epsilon_yukawa; 
  const vec difference = (loc1-loc2);   /// 3 FLOP | register += 3*4 = 12

  // + vec(1.0f, 0.0f, 0.0f)*epsilon*(loc1.x==loc2.x && loc1.y==loc2.y && loc1.z==loc2.z);
  // this checks if both locations are equal (this would lead to an infint value for the
  // coulomb force.  If they are equal, a small value is geiven as difference
  // however this will cause the particles to always stick together
  // which is not physically correct - need some particle dependet value
  // ALTERNATIVE: Yukawa potential
  //return difference * (coulomb/(util::cube(sqrt(util::square<vec, numtype>(difference) + 1e-10f))));

  // used here: Yukawa potential
  return difference *
    (coulomb / (util::cube(sqrt( util::square(difference.x) + 
     util::square(difference.y) + util::square(difference.z) + epsilon ))) ); /// 19 FLOP

}




#endif
